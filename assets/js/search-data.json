{
  
    
        "post0": {
            "title": "Visualizing deep learning with galaxies, part 2",
            "content": "Galaxies, neural networks, and interpretability . Up to the point, we have been interested in predicting a galaxy&#39;s elemental abundances from optical-wavelength imaging. Using the fast.ai library, we were able to train a deep CNN and estimate metallicity to incredibly low error in under 15 minutes. We then used dimensionality reduction techniques to help visualize the latent structure of CNN activations, and identified how morphological features of galaxies are associated with higher or lower metallicities. . . In this post, we will look more closely at the CNN activation maps to see which parts of the galaxies are associated with predictions of low or high metallicity. This method of interpretation is sometimes referred to as image attribution. . One key difference between this analysis and the previous ones is that we will definine the CNN task as a binary classification problem rather than a regression problem. Once we have trained the classifier to distinguish low- and high-metallicity galaxies, we will be able to produce activation maps for both classes, even though the CNN will only predict one of the two. Setting up this classification task is no more difficult than the previous regression problem using the fastai DataBlock API. . Note: The fastai v2 library has been officially released, so definitely try it out if you haven&#8217;t yet! I&#8217;ve previously referred to this as the fastai2 library, but now it can be found in the main repository: https://github.com/fastai/fastai. . Binning the galaxies into metallicity classes . First, we need to define the classes using the parent data set. Below, I plot a histogram of the metallicities for the entire galaxy catalog, where we can see that the mean of the distribution is about $Z = 8.9$. . There&#39;s no obvious way to define metallicity &quot;classes&quot; since the distribution is unimodal and smooth. We can use pd.cut to sort low, medium, and high metallicities into bins $(8.1, 8.7]$, $(8.7, 9.1]$, and $(9.1, 9.3]$. . df[&#39;Z&#39;] = pd.cut( df.metallicity, bins=[8.1, 8.7, 9.1, 9.3], labels=[&#39;low&#39;, &#39;medium&#39;, &#39;high&#39;] ) df.Z.value_counts() . medium 97097 low 21403 high 15421 Name: Z, dtype: int64 . The majority are labeled as medium metallicites, but we will dropping things, such that our remaining data comprises two well-separated classes. The remaining low- and high-metallicity galaxies have slightly imbalanced classes, but this imbalance isn&#39;t be severe enough to cause any issues. (In more problematic cases, we could try resampling or weighting our DataLoaders, or weighting the cross entropy loss.) . df = df[df.Z != &#39;medium&#39;] df.Z.value_counts() . low 21403 high 15421 medium 0 Name: Z, dtype: int64 . A CNN classification model . DataBlocks for classification . Now that we have a smaller DataFrame with a column of metallicity categories (Z), we can construct the DataBlock. There are a few notable differences between this example and the previous DataBlock for regression: . we use CategoryBlock rather than RegressionBlock as the second argument to blocks | we supply ColReader(&#39;Z&#39;) for get_y | we have zoomed in on the images and only use the central 96×96 pixels, which will allow us to interpret the activation maps more easily | . Afterwards, we populate ImageDataLoaders with data using from_dblock(). . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), get_x=ColReader(&#39;objID&#39;, pref=f&#39;{ROOT}/images/&#39;, suff=&#39;.jpg&#39;), get_y=ColReader(&#39;Z&#39;), splitter=RandomSplitter(0.2, seed=seed), item_tfms=[CropPad(96)], batch_tfms=aug_transforms(max_zoom=1., flip_vert=True, max_lighting=0., max_warp=0.) + [Normalize], ) dls = ImageDataLoaders.from_dblock(dblock, df, path=ROOT, bs=64) . We can show a few galaxies to get a sense for what these high- and low-metallicity galaxies look like, keeping in mind that many &quot;normal&quot; spiral galaxies with typical metallicities have been excluded. . dls.show_batch(max_n=8, nrows=2) . Constructing a simple CNN . Next, we will construct our model. We will use the fast.ai ConvLayer class instead of writing out each sequence of 2d convolution, ReLU activation, and batch normalization layers. After the ConvLayers, we pool the activations, flatten them so that they are of shape (batch_size, 128), and pass them through a fully-connected (linear) layer. . model = nn.Sequential( ConvLayer(3, 32), ConvLayer(32, 64), ConvLayer(64, 128), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(128, dls.c) ) . That&#39;s it! We have a tiny 4-layer (not counting the pooling and flattening operations) neural network! Since there are only two classes, the DataLoaders knows that dls.c = 2 (even though there was a third class, galaxies with medium metallicities, but we&#39;ve removed all of those examples from the catalog). . This final linear layer will output two floating point numbers. Although they might take on values outside the interval $[0, 1]$, they can be converted into probabilities by using the softmax function, and this is done implicitly as part of the nn.CrossEntropyLoss, which we will cover below. . Optimization and metrics . We can create a fast.ai Learner object just like before. Since we are working on a classification problem, the Learner assumes that we want a flattened version ofnn.CrossEntropyLoss. Thus, the argument to loss_func is optional (unlike in the previous the regression problem, where we needed to specify RMSE as the loss function). In this example, we do also have the option of passing in a weighted or label-smoothing cross entropy loss function, but it&#39;s not necessary here. . Cross entropy loss is great because it&#39;s the continuous, differentiable, negative log-likelihood of the class probabilities. On the flip side, it&#39;s not obvious how to interpret this loss function; we&#39;re more accustomed to seeing the model accuracy or some other metric. Fortunately, we can supply additional metrics to the Learner in order to monitor the model&#39;s performance. One obvious metric is the accuracy. We can also look at the area under curve of the receiving operator characteristic and the $F_1$ score (RocAuc and F1Score, respectively, in fast.ai). If we pass . metrics = [accuracy, RocAuc(), F1Score()] . to the Learner constructor, these metrics will be printed for the validation set after every epoch of training. . learn = Learner( dls, model, opt_func=ranger, metrics=[accuracy, RocAuc(), F1Score()] ) . Cool! Now let&#39;s pick a learning rate (LR) and get started. By the way, shallower models tend to work better with higher learning rates. So it shouldn&#39;t be a surprise that the LR finder identifies a higher LR than before (where we used a 34-layer xresnet). . learn.lr_find() . SuggestedLRs(lr_min=0.05248074531555176, lr_steep=1.5848932266235352) . We can fit using the one-cycle (fit_one_cycle()) schedule as we did before. Here I&#39;ve chosen 5 epochs just to keep it quick. . Note: Often the fit_flat_cos() scheduler works well for classification problems (and not regression problems). It might be worth a shot if you&#8217;re training a model from scratch — but if you&#8217;re using transfer learning, then I recommend sticking to fit_one_cycle(), since the &quot;warmup phase&quot; seems to be necessary for good results. . learn.fit_one_cycle(5, 8e-2) . epoch train_loss valid_loss accuracy roc_auc_score f1_score time . 0 | 0.340562 | 0.349320 | 0.903313 | 0.915260 | 0.909530 | 00:33 | . 1 | 0.268380 | 0.587023 | 0.828218 | 0.800253 | 0.868215 | 00:33 | . 2 | 0.259624 | 0.212257 | 0.935361 | 0.930546 | 0.945098 | 00:33 | . 3 | 0.244578 | 0.197691 | 0.945817 | 0.945994 | 0.952809 | 00:33 | . 4 | 0.227754 | 0.185524 | 0.950842 | 0.950158 | 0.957412 | 00:33 | . In three minutes of training, we can achieve 95% in accuracy, ROC area-under-curve, and $F_1$ score. We can certainly do better (&gt;98% for each of these metrics) if we trained for longer, used a deeper model, or leveraged transfer learning, but this performance is sufficient for revealing further insights. After all, we want to know which morphological features are responsible for the low- and high-metallicity predictions. Indeed, shallower neural networks with fewer pooling layers produce activation maps that are easier to interpret! . Finally, I would be remiss if I didn&#39;t mention that fast.ai offers a ClassificationInterpretation module! It can be used to plot a confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() plt.xlabel(r&#39;Predicted $Z$&#39;, fontsize=12) plt.ylabel(r&#39;True $Z$&#39;, fontsize=12); . ClassificationInterpretation can also plot the objects with highest losses, which is helpful for diagnosing what your model got wrong. Not only that, but it also has the Grad-CAM visualization baked in, so that you can visualize exactly which parts of the image it has gotten incorrect. But in the next section, we will implement Grad-CAM ourselves using Fastai forward and backwards hooks. If you&#39;re unfamiliar with this topic, it could be helpful to refer to the Callbacks and Hooks section of my previous post before proceeding to the next section. . Explaining model predictions with Grad-CAM . Grad-CAM and visual attributions . We now have a CNN model trained to recognize low- and high-metallicity galaxies. If the model is given an input image of a galaxy, we can also see which parts of the image &quot;light up&quot; with activations based on the galaxy features that it has learned. This method is called class activation mapping (see Tong et al. 2015). . We might expect the CNN to rely on different morphological features for recognizing different classes. If these essential features are altered, then the classification might change dramatically. Therefore, we need to look at features for which the gradient (corresponding to a given feature) is large, and this can be accomplished by visualizing the gradient-weighted class activation map (Grad-CAM). This work is detailed in &quot;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization,&quot; by Selvaraju et al. (2016). . . Hooks for storing activations and gradients . Pytorch automatically computes gradients during the backwards pass for each (trainable) layer. However, it doesn&#39;t store them, so we need to make use of the hook functionality in order to save them on the forward pass (activations) and backward pass (gradients). The essential Pytorch code is shown below (adapted from the Fastai book). . class HookActivation(): def __init__(self, target_layer): &quot;&quot;&quot;Initialize a Pytorch hook using `hook_activation` function.&quot;&quot;&quot; self.hook = target_layer.register_forward_hook(self.hook_activation) def hook_activation(self, target_layer, activ_in, activ_out): &quot;&quot;&quot;Create a copy of the layer output activations and save in `self.stored`. &quot;&quot;&quot; self.stored = activ_out.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() class HookGradient(): def __init__(self, target_layer): &quot;&quot;&quot;Initialize a Pytorch hook using `hook_gradient` function.&quot;&quot;&quot; self.hook = target_layer.register_backward_hook(self.hook_gradient) def hook_gradient(self, target_layer, gradient_in, gradient_out): &quot;&quot;&quot;Create a copy of the layer output gradients and save in `self.stored`. &quot;&quot;&quot; self.stored = gradient_out[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . Note that the two classes are almost the same, and that all of the business logic can be boiled down to: . define a hook function (e.g., hook_gradient) that captures the relevant output from a model layer | register a forward or backward hook using this function | define a Python context using __enter__ and __exit__ so that we don&#39;t waste memory and can easily call the hooks like with(HookGradient) as hookg: [...] | We&#39;re interested in the final convolutional layer, as the early layers may have extremely vague features that that may not correspond specifically to any one class. . target_layer = learn.model[-4] learn.model . Sequential( (0): ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): AdaptiveAvgPool2d(output_size=1) (4): Flatten(full=False) (5): Linear(in_features=128, out_features=2, bias=True) ) . A test image . We also need to operate on a single image at a time. (I think we can technically use a mini-batch of images, but then we&#39;ll end up with a huge tensor of gradients!) Let&#39;s target this nice-looking galaxy. . img = PILImage.create(f&#39;{ROOT}/images/1237665024900858129.jpg&#39;) img.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f432c10a490&gt; . We can see that the model is incredibly confident that this image is of a high-metallicity galaxy. . learn.predict(img) . (&#39;high&#39;, tensor(0), tensor([1.0000e+00, 7.1970e-19])) . However, learn.predict() is doing a lot of stuff under the hood, and we want to attach hooks to the model while it&#39;s doing all that. So we&#39;ll walk through this example step-by-step. . First, we need to apply all of the item_tfms and batch_tfms (like cropping the image, normalizing its values, etc) to this test image. We can put this image into a batch and then retrieve it (along with non-existent labels) using first(dls.test_dls([img])). . We use dls.train.decode() to process these transforms, and pass it (the first element, and first batch) into a TensorImage which can be shown the same was as a PILImage. . x, = first(dls.test_dl([img])) x_img = TensorImage(dls.train.decode((x,))[0][0]) x_img.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f432e7c0890&gt; . Next, we want to generate the Grad-CAM maps. We can produce one for each class, so let&#39;s double-check dls.vocab to make sure we know the mapping between integers and high or low metallicity classes. It turns out that 0 corresponds to high, 1 corresponds to low. . dls.vocab . (#2) [&#39;high&#39;,&#39;low&#39;] . At this point, we can simply apply the hooks and save the stored values into other variables. . During the forward pass, we want to put the model into eval mode and stick the image onto the GPU: learn.model.eval()(x.cuda()). We can then save the activation in act. | We then need to do a backwards pass to compute gradients with respect to one of the class labels. If we want gradients with respect to the low-metallicity class, then we would call output[0, 1].backward() (note that this 0 references the lone example in the mini-batch). We can store the gradient in grad. | We might also find it helpful to get the class probabilities, which we temporarily saved in output. We can get rid of their gradients and store the two values in p0 and p1, the low-z and high-z probabilities (which sum up to one). | . # low-metallicity class_Z = 1 with HookGradient(target_layer) as hookg: with HookActivation(target_layer) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0, class_Z].backward() grad = hookg.stored p0, p1 = output.cpu().detach()[0] . Finally, computing the Grad-CAM map is super easy! We average the gradients across the spatial axes (leaving only the &quot;feature&quot; axis) and then take the inner product with the activation maps. In the language of mathematics, we are computing $$ sum_{k} frac{ partial y}{ partial mathbf{A}^{(k)}_{ij}} left [ frac{1}{N_i N_j} sum_{i,j} mathbf{A}^{(k)}_{i,j} right ],$$ for the $k$ feature maps, $ mathbf{A}^{(k)}_{i,j}$, and the target class $y$. Note that the feature maps have shape $N_i times N_j$, which ends up in the denominator as a constant, but this just gives us an arbitrary scaling factor. Finally, we stop Pytorch from computing any more gradients and pop it off the GPU with .detach() and .cpu(). We can then plot it below. . w = grad[0].mean(dim=(1,2), keepdim=True) gradcam_map = (w * act[0]).sum(0).detach().cpu() . Interesting! Looks like it has highlighted the outer regions of the galaxy. Let&#39;s also visualize the high-metallicity parts of the image using the same exact code (except, of course, switching class_Z = 0 to class_Z = 1). . Putting it together . Cool, so now we know how this all works! However, we should actually take only the positive contributions of the Grad-CAM map, because activations are passed through a ReLU layer in the CNN. We can do this by calling torch.clamp(). Since matplotlib imshow() rescales the colormap anyway, the result is that we&#39;ll see less of the lower-valued (darker) portions of the Grad-CAM map, but the higheest-valued (brighter) parts will not change. . We will shove all this into a function, plot_gradcam, which computes the Grad-CAM maps for low and high metallicity labels, organizes the matplotlib plotting, and returns the figure, axes, and CNN probabilities. . def plot_gradcam(x, learn, hooked_layer, size=96): fig, axes = plt.subplots(1, 3, sharey=True, figsize=(8.5, 3), dpi=150) x_img = TensorImage(dls.train.decode((x,))[0][0]) for i, ax in zip([0, 2, 1], axes): if i == 0: x_img.show(ax=ax) ax.set_axis_off() continue with HookGradient(hooked_layer) as hookg: with HookActivation(hooked_layer) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0, i-1].backward() grad = hookg.stored p_high, p_low = output.cpu().detach()[0] w = grad[0].mean(dim=(1,2), keepdim=True) gradcam_map = (w * act[0]).sum(0).detach().cpu() # thresholding to account for ReLU gradcam_map = torch.clamp(gradcam_map, min=0) x_img.show(ax=ax) ax.imshow( gradcam_map, alpha=0.6, extent=(0, size, size,0), interpolation=&#39;bicubic&#39;, cmap=&#39;inferno&#39; ) ax.set_axis_off() fig.tight_layout() fig.subplots_adjust(wspace=0.02) return (fig, axes, *(np.exp([p_low, p_high]) / np.exp([p_low, p_high]).sum())) . And now we can plot it! It looks much better now that we&#39;ve applied the ReLU. I have also added a few extra captions so that we can see the object ID and CNN prediction probabilities. . We can see not only why the CNN (confidently) classified this galaxy as a high-metallicity system, i.e. its bright central region, but also which parts of the image were most compelling for it to be classified as a low-metallicity galaxy, even though it didn&#39;t make this prediction! Here, we see that it has highlighted the far-outer blue spiral arms of this galaxy. . A few more examples . Since we&#39;ve invested this effort into making the plot_gradcam() function, let&#39;s generate some more pretty pictures. We can grab some random galaxies from the validation set between the redshifts 0.05 &lt; z &lt; 0.08 (i.e., typical galaxy redshifts), and process them using the trained CNN and Grad-CAM. . val_df = dls.valid.items objids = val_df[(0.05 &lt; val_df.z) &amp; (val_df.z &lt; 0.08)].sample(5, random_state=seed).objID . Conclusions . I hope that you&#39;ve enjoyed this journey through data visualization techniques using fast.ai! One of the goals was to convince you that convolutional neural networks can be interpretable, and that methods like Grad-CAM are crucial for understanding what a CNN has learned. Since the neural network makes more accurate predictions than any human, we can gain invaluable knowledge by observing what the model focuses on, potentially leading to new insights in astronomy! . If you&#39;re interested in some academic discussion of this sort of topic, then I encourage you to check out my most recent paper, &quot;Connecting optical morphology, environment, and HI mass fraction for low-redshift galaxies using deep learning&quot;, which delves into a closely related topic. In this work, I use pattern recognition classifier combined with a highly optimized CNN regression model to estimate the gas content of galaxies with state-of-the-art results! Grad-CAM makes an appearance in Figure 11, and is even used for visual attribution in monochromatic images (see below). The paper has just been accepted to the Astrophysical Journal (ApJ), and is currently in press, but you can view the semi-final version on arXiv now! . .",
            "url": "https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/visualization/2020/08/27/image-attribution-for-galaxies.html",
            "relUrl": "/galaxies/astrophysics/deep%20learning/visualization/2020/08/27/image-attribution-for-galaxies.html",
            "date": " • Aug 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Visualizing deep learning with galaxies, part 1",
            "content": "Getting started . Interested in understanding the inner workings of a trained convolutional neural network (CNN) model? There is good reason to believe that deep learning algorithms don&#39;t always work the way we want or expect, so it would be nice to have a way to verify our intuitions. Also, if we want to learn physics from images of the cosmos, then we need a way to connect the pretty pictures with our understanding of the physical processes at work. One way to do this is by using our trained model to map the inputs (images) to a vector space of morphological features (sort of like a latent representation). We can then explore this feature vector space by plotting (hello matplotlib) with the help of standard dimensionality reduction tools like PCA, or more sophisticated methods if we&#39;re adventurous. Let&#39;s get started! . First, we will load our galaxy metallicity data set from a previous exercise. Since we&#39;re going to be working with the trained CNN from before, it might be a good idea to check out that post if the next few code blocks seem unfamiliar. . df = pd.read_csv(f&#39;{ROOT}/data/master.csv&#39;, dtype={&#39;objID&#39;: str}).rename({&#39;oh_p50&#39;: &#39;metallicity&#39;}, axis=1) dblock = DataBlock( blocks=(ImageBlock, RegressionBlock), get_x=ColReader([&#39;objID&#39;], pref=f&#39;{ROOT}/images/&#39;, suff=&#39;.jpg&#39;), get_y=ColReader([&#39;metallicity&#39;]), splitter=RandomSplitter(0.2), item_tfms=[CropPad(144), RandomCrop(112)], batch_tfms=aug_transforms(max_zoom=1., flip_vert=True, max_lighting=0., max_warp=0.) + [Normalize], ) dls = ImageDataLoaders.from_dblock(dblock, df, path=ROOT, bs=256) def root_mean_squared_error(p, y): return torch.sqrt(F.mse_loss(p.reshape(-1), y.reshape(-1))) . We will also load our trained model. If it didn&#39;t get saved last time, that&#39;s okay! The model only takes 15 minutes to train, and you can just substitute learn.fit_one_cycle(7, 1e-2) instead of learn.load(). . model = xresnet34(n_out=dls.c, sa=True, act_cls=MishCuda) learn = Learner( dls, model, opt_func=ranger, loss_func=root_mean_squared_error ) learn = learn.load(&#39;xresnet34-1&#39;); . Grab the CNN activations . Computing predictions . Now that we have loaded up our trained CNN model, we can simply use learn.get_preds() to compute metallicity predictions. This will iterate through our entire validation set (or another speicified Pytorch Dataset) and return a tuple of predictions and ground truths. . Z_pred, Z_true = learn.get_preds(dl=dls.valid, reorder=False) . For convenience, I like to stick these into a pandas DataFrame object, indexed by their objID. That way, it&#39;s easy to keep track of each galaxy&#39;s filename (and image), target metallicity, and predicted metallicity. . results = pd.DataFrame( { &#39;true&#39;: Z_true.view(-1), &#39;pred&#39;: Z_pred.view(-1), }, index=dls.valid.items.objID.astype(np.int64) ) . Another convenient way to work with our data is by using the generator functionality in a Dataset. Calling next(iter(Dataset)) yields a batch of inputs and a batch of targets, which I have named xb and yb respectively. We can verify that xb contains 256 images, each of which have 3 channels and 112✕112 pixels. . xb, yb = next(iter(dls.valid)) xb.shape . We can also quickly verify that these are the same data as the first 256 sources retrieved by the previous learn.get_preds() method. . np.allclose(yb.cpu(), results.true[:256]) . Next, we will need to select a CNN layer or module of interest. In this post, we will fetch activations prior to the final linear layer, which is equivalent to the final convolutional feature maps passed through an adaptive pooling layer. These 512 activations comprise a high-level morphological feature vector for each galaxy. . Callbacks and hooks . Fastai provides a Callback functionality to implement certain actions while your model is being evaluated or trained. You can do really powerful things with Callbacks, like dynamically shift between half (16 bit) and full (32 bit) floating-point precision or change the learning rate and weight decay over the course of a training loop. . We are interested in grabbing (hooking) activations from a layer module in the neural network. How do we retrieve these activations? With a special fastai Callback: hook_output(), which stores the layer&#39;s activations during the forward pass. By the way, there are other Callbacks that we can use, but I won&#39;t go into details. For more examples of hooks, check out the Pytorch examples, the fastai2 documentation, or Chapter 18 of the fastai book. In a future post, we will deploy backward hooks that also store the gradients during backpropagation! . Although we have already computed predictions once, we didn&#39;t hook the activations! We will need to do so again, this time with hook_output() in place. I do this using the model in eval() mode, which ensures that batch normalization works with the saved statistics, and not batch statistics. Note that dropout is also disabled during eval() mode, but by default xresnet turns off dropout layers (and besides, dropout occurs after the currently hooked layer). . m = learn.model.eval() . with hook_output(learn.model[-3], cpu=True) as hook: outputs = m(xb) acts = hook.stored . Dimensionality reduction with PCA . Great, so we now have the CNN activations for 256 galaxies. We should detach them from the computational graph using .detach(), as we&#39;ll no longer need Pytorch to automatically compute gradients. We can also send the activations to the CPU using .cpu(). Finally, let&#39;s convert these into Numpy arrays by calling .numpy(). Below, I have chained together these operations in a slightly different order. . acts = acts.detach().cpu().numpy() acts.shape . Now we&#39;ve got 256 galaxies described 512-dimensional feature vectors loaded up on our CPU. There are too many activations to view one-by-one, so we need to find a way to condense this information via dimensionality reduction techniques. . Fortunately, there are several ways to reduce the dimensionality of these data, and the most well-known method is principal components analysis (PCA). There are many excellent guides and tutorials that explain PCA better than I can, so it&#39;s worth searching around if this is an unfamiliar concept. I highly recommend the Essence of Linear Algebra series by 3Blue1Brown. Since I like to think of matrices as coordinate transformations, PCA can be thought of as a way to identify the $d times r$ matrix that projects your $d$-dimensional data (in this case $d = 512$) onto the most important, independent, $r$-dimensional feature vectors. By most important, I mean the largest amount of variance along that dimension. . Scikit-learn provides a very easy-to-use implementation of PCA (sklearn.decomposition.PCA). We will select $r = 3$ so that we can easily visualize it later; this is specified using the n_components argument of the PCA() constructor. . pca = PCA(n_components=3, whiten=True) acts_PCA = pca.fit_transform(acts_flattened) acts_PCA.shape . pca.explained_variance_ratio_.sum() . These three dimensions are responsible for 98% of the variance! If we want to see how much variance we have retained using only the first two principal components, then we can run pca.explained_variance_ratio_[:2].sum() -- it turns out to be about 97%. In fact, 92% of the variance can be explained solely using the first component! . As a side note, we can also use $t$-distributed Stochastic Neighbor Embedding (t-SNE) to visualize the activations. There are some wonderful guides on t-SNE, and I strongly recommend this one by distill.pub. It is also supported in Scikit-learn and easy to use. However, t-SNE can be a bit tricky to interpret, its output depends considerably on hyperparameter choices, and there is some stochasticity in its results. Thus, I&#39;ll skip t-SNE for now, but might come back to it in another post. . Visualization with matplotlib . Let&#39;s see a scatterplot of the data transformed along its first two PCA dimensions. I&#39;ll also color each point: the inside color will correspond to the predicted metallicity, and the outside color will correspond to the target metallicity. A colorbar will give us a sense of scale; the metallicity, defined as 12 + log(O/H), typically ranges between 8.2 and 9.4 for this galaxy sample. . It&#39;s nice to see that the metallicity varies quite smoothly in this space! However, it&#39;s hard to see any details with all these galaxies piled up on top of each other. . Note: The Nike swoosh curve in the scatter plot is interesting from an astrophysics standpoint. The metallicity appears to be double-valued for PCA $ hat{x}_2$, not dissimilar to the degenerate metallicity solutions from the $R_{23}$ indicator (Pagel et al. 1979). However, this is not the cause of the shape here, as $R_{23}$ becomes double-valued at a metallicity of about 8.0, whereas the turnover here is observed at 8.8. . We can also plot the galaxy points using PCA dimension $ hat{x}_3$: . Now the points aren&#39;t piled up on top of each other, but the metallicity appears to only have a gradient in the PCA $ hat{x}_1$ dimension. . Finally, rather than representing each galaxy as a colored point, we can display an actual image of the galaxy. Using the AnnotationBbox and OffsetImage classes in matplotlib.offsetbox, we can plop down galaxy images in the scatter plot. We can also color the box around each image according to its metallicity using the bboxprops keyword. . Now we can really see some morphological stratification of this galaxy subsample! From right to left, it is evident that galaxies have progressively brighter and redder central regions (bulges), and this coincides with a rise in metallicity. In the lower-left corner, galaxies have prominent spiral arms, blue disks, and red bulges. In the upper center portion of this figure, galaxies are mostly inclined (i.e., viewed edge-on). . Conclusions . Simple methods such as PCA can be powerful for understanding what a neural network has learned. Although we already knew that a trained CNN could robustly predict metallicity, we now can see that it has learned how to distinguish high- and low-metallicity galaxies on the basis of their colors, spiral arms/disks, bulges, and viewing angle. Indeed, we also could have deduced this by simply visualizing the highest and lowest metallicity galaxies, as was shown in the first blog post: . . In Part 2 of this series, we&#39;ll continue looking into deep learning visualization techniques. We will be using Gradient-weighted Class Activation Maps, or Grad-CAM, to localize the image features that a CNN focuses on when it classifies a galaxy as metal-rich or metal-poor. .",
            "url": "https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/visualization/2020/07/27/galaxy-feature-space.html",
            "relUrl": "/galaxies/astrophysics/deep%20learning/visualization/2020/07/27/galaxy-feature-space.html",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Identifying racial bias in health data",
            "content": ". Note: Following the change in the AP Stylebook, and many others who have been advocating it for years, I will be capitalizing the b in Black when referring to the racial, cultural, and ethnic identity. . I recently read the incredible paper, Dissecting racial bias in an algorithm used to manage the health of populations, by Obermeyer et al. (2019). The authors analyzed an algorithm that was commonly used for evaluating patients&#39; health risks and determining which patients should be enrolled in a care management program. The authors compared the algorithm&#39;s risk score with a number of patient outcomes, including medical costs and total number of chronic illnesses. They found evidence of racial bias in the algorithm, which could lead to worse health outcomes for Black patients than white patients given the same algorithm risk score. . Because of US privacy law concerning protected health information, or PHI, the study was not able to publish the original hospital patient data. Instead, they released a synthetic data set generated with the synthpop R package. These data (and tools for their analysis) can be found in their Gitlab repo. . . Note: I try to recreate some of the authors&#8217; results solely based on the synthetic data set, and by examining their paper and figures. They do have code available for reproducing the analysis, but since I didn&#8217;t look at it, my quick exploratory data analysis may differ from their results. Also, the synthetic data may not agree perfectly with the original data. . A (synthetic) health care data set . First, let&#39;s take a look at the data. The data README was particularly helpful for understanding the contents of the table. For example, columns ending in _t are measures of outcomes during time $t$, and columns ending in _tm1 are data from the preceding year (including comorbidity and biomarker indicators used for training the algorithm). Here are a few rows from the data set, read in from the CSV using pandas: . df = pd.read_csv(f&#39;{ROOT}/data/data_new.csv&#39;) df.sample(3) . risk_score_t program_enrolled_t cost_t cost_avoidable_t bps_mean_t ghba1c_mean_t hct_mean_t cre_mean_t ldl_mean_t race ... trig_min-high_tm1 trig_min-normal_tm1 trig_mean-low_tm1 trig_mean-high_tm1 trig_mean-normal_tm1 trig_max-low_tm1 trig_max-high_tm1 trig_max-normal_tm1 gagne_sum_tm1 gagne_sum_t . 30989 0.594530 | 0 | 2700.0 | 0.0 | 110.0 | NaN | 37.9 | NaN | NaN | white | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | . 36979 2.242229 | 0 | 900.0 | 0.0 | 126.0 | NaN | NaN | NaN | NaN | white | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 2 | . 29424 2.021403 | 0 | 51200.0 | 0.0 | 126.0 | NaN | 42.2 | 0.94 | NaN | white | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | . 3 rows × 160 columns . We will work with the Black and white populations separately in order to probe any racial disparity in health outcomes. . white = df[df.race == &#39;white&#39;] black = df[df.race == &#39;black&#39;] len(white), len(black) . (43202, 5582) . We can see that about 88% of the patients are white. Patients who identify as another race are not included in the data. Note that the same patient can appear multiple times in the original data, as each row represents a single year of patient data; I haven&#39;t made any statistical corrections for this. . No racial disparity in medical costs . The algorithm is trained to predict patients&#39; future medical costs as a way of identifying those who would benefit from enrolling in a health care coordination program. Future medical costs are frequently used as the objective function (think negative loss function) for training such algorithms; this algorithm seeks to identify patients with the highest predicted costs. The authors point out that other objectives (or &quot;labels&quot;) can be used as well. . Next, I try to recreate the results shown in Figure 3a of Obermeyer et al. (2019): . . The race-disaggregated patients are binned by risk score percentile, and then the mean medical cost is computed for each bin. Note the logarithmic y-axis. These kinds of plots seem to be common in economics (for example, the Opportunity Insights research by Raj Chetty and collaborators), although I rarely see data illustrated this way in physics or astronomy. . Fortunately, with pandas, we can bin the risk score by quantile using pd.qcut(). We then feed the output into the powerful DataFrame.grouby() function in order to get binned statistics, from which we compute the average mean cost. . bins = np.arange(1, 100, 2) white_cost_means = white.groupby(pd.qcut(white.risk_score_t, q=50)).cost_t.mean() / 1e3 black_cost_means = black.groupby(pd.qcut(black.risk_score_t, q=50)).cost_t.mean() / 1e3 . Here is a scatter plot of the mean medical costs, in thousands of US dollars, as a function of risk score percentile. . Qualitatively, we find the same results as the original paper. This is no evidence for racial disparity so far, but this result should not surprise us. The y-axis is the training objective for the algorithm that also produces the risk score, and race data are excluded from these predictions. Thus, it is expected that no explicit racial bias appears... yet. . Number of chronic illnesses as a health outcome . So far so good, right? But wait, medical cost is not the only outcome that we should care about. Health outcomes are arguably more important than medical costs, and one way to measure health outcomes is to count how many chronic conditions a patient has. . Below, I plot histograms of the number of active chronic illnesses during year $t$ for white and Black patients. (For some reason, this column is named gagne_sum_t in the CSV file.) Most patients have zero or one active chronic conditions, and are exponentially less likely to have more than that. . Revealing racial disparity in health outcomes . Let&#39;s now plot the average number of chronic illnesses at given risk score percentile for Black and white patients. . white_gagne_means = white.groupby(pd.qcut(white.risk_score_t, q=50)).gagne_sum_t.mean() black_gagne_means = black.groupby(pd.qcut(black.risk_score_t, q=50)).gagne_sum_t.mean() . There is now a noticeable gap between Black and white patients. And according to the analysis by Obermeyer et al., this gap (also seen in their Figure 1a) is statistically significant. So what are the implications of this disparity? . The original purpose of the algorithm in question was to determine who to enroll into a care coordination program. Black patients only made up 17.7% of the population based on the medical cost metric. If instead the algorithm were trained to achieve parity in mean number of chronic illnesses, then 46.5% of patients identified for enrollment would be Black. In essence, Black patients with pressing medical needs were systematically left out due to the algorithmic choice of medical cost as its objective function. . Higher medical expenditures for white patients . If Black and white patients have similar average medical costs given the same level of risk, then why do they have disparate health outcomes? One possible explanation is that white patients generate higher costs than Black patients even if they are equally healthy. . Obermeyer et al. probe this mechanism by plotting the mean medical costs by number of chronic conditions, shown in their Figure 3b: . . Let&#39;s see if we can replicate this! However—again, having not looked at their code—I am not sure how they bin the number of chronic conditions by percentile, given that over 30% of the patients have zero chronic conditions. . We can still explore the data in a similar way: by plotting the mean medical costs for white and Black patients, conditioned on the number of chronic illnesses. . gagne_bins = pd.interval_range(start=0, end=8, freq=1, closed=&#39;left&#39;) white_costs_by_gagne = white.groupby(pd.cut(white.gagne_sum_t, gagne_bins, include_lowest=True)).cost_t.mean() / 1e3 white_num_by_gagne = pd.cut(white.gagne_sum_t, gagne_bins).value_counts() black_costs_by_gagne = black.groupby(pd.cut(black.gagne_sum_t, gagne_bins, include_lowest=True)).cost_t.mean() / 1e3 black_num_by_gagne = pd.cut(black.gagne_sum_t, gagne_bins).value_counts() . This plot shows the same qualitative trend as the original paper&#39;s result. For the majority of Black and white patients with zero or one chronic illnesses, there are substantial disparities in medical costs, whereby Black patients generate lower medical costs. . I want to leave a quote from the original paper (emphasis mine) that demonstrates how pernicious this effect can be. . To summarize, we find substantial disparities in health conditional on risk but little disparity in costs. On the one hand, this is surprising:Health care costs and health needs are highly correlated, as sicker patients need and receive more care, on average. On the other hand, there are many opportunities for a wedge to creep in between needing health care and receiving health care—and crucially, we find that wedge to be correlated with race [...]. . Anti-racism and mitigating bias in machine learning . We see a clear picture of how racial disparities can manifest due to choice of objective function. The machine learning algorithm was trained to identify the most expensive patients based on comorbidity variables and biomarkers, as is commonly done in the health care world (and, in this case, the algorithm was developed by an &quot;industry leader in data and analytics&quot;). We see that patients&#39; algorithm risk scores correlate with their medical expenditures during the following year, and that there is no racial disparity in this correlation. However, medical costs are not necessarily good proxies for health outcomes. If we examine the number of chronic illnesses as a function of algorithm risk score instead, then it becomes clear that Black patients have worse health outcomes at any given predicted risk score. Obermeyer et al. (2019) also look at other health outcomes (e.g., diabetes severity), and find similar racial biases. They describe the problem succinctly in their paper&#39;s abstract: . The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. . One reason for this racial disparity may be that white patients have higher medical costs than Black patients given the number of active chronic illnesses. These could be caused or compounded by inequitable access to health care and unequal treatment at health care facilities (see, e.g., brief results from a notebook I made to explore this based on 2013 Medicare Current Beneficiary Survey data). We also know that inequitable distribution of medical resources is causing health disparities during the coronavirus pandemic even now (i.e., African Americans showing COVID-19 symptoms are less likely to receive treatment). . Fortunately, the authors present a solution, at least for the biased algorithm in the study. Instead of training the algorithm on medical costs, they use a combination of costs and health predictors as the training target and (at time of publication) are able to reduce racial bias by 84%. The takeaway is that it is possible—and thus ethically imperative—to reduce bias in machine learning algorithms, even as we make every effort to rectify human biases. It is a difficult task, of course, and data scientists will need to partner with domain experts in order to understand the many ways that algorithmic bias can appear. By exploring the data and forming teams with diverse backgrounds, we will be better equipped to handle biased data and correct biased algorithms. .",
            "url": "https://jwuphysics.github.io/blog/big%20data/health/ethical%20ai/machine%20learning/2020/06/11/identifying-racial-bias-in-health-data.html",
            "relUrl": "/big%20data/health/ethical%20ai/machine%20learning/2020/06/11/identifying-racial-bias-in-health-data.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Training a deep CNN to learn about galaxies in 15 minutes",
            "content": "Predicting metallicities from pictures: obtaining the data . In my previous post, I described the problem that we now want to solve. In short, we want to train a convolutional neural network (CNN) to perform regression. The inputs are images of individual galaxies (although sometimes we&#39;re photobombed by other galaxies). The outputs are metallicities, $Z$, which usually take on a value between 7.8 and 9.4. . The first step, of course, is to actually get the data. Galaxy images can be fetched using calls to the Sloan Digital Sky Survey (SDSS) SkyServer getJpeg cutout service via their RESTful API. For instance, this URL grabs a three-channel, $224 times 224$-pixel JPG image: . Galaxy metallicities can be obtained from the SDSS SkyServer using a SQL query and a bit of JOIN magic. All in all, we use 130,000 galaxies with metallicity measurements as our training + validation data set. . The code for the original published work (Wu &amp; Boada 2019) can be found in my Github repo. However, this code (from 2018) used fastai version 0.7, and I want to show an updated version using the new and improved fastai2 codebase. Also, some of the &quot;best practices&quot; for deep learning and computer vision have evolved since then, so I&#39;d like to highlight those updates as well! . Organizing the data using the fastai2 DataBlock API . Suppose that we now have a directory full of galaxy images, and a csv file with the object identifier, coordinates, and metallcity for each galaxy. The csv table can be read using Pandas, so let&#39;s store that in a DataFrame df. Let&#39;s take a look at five random rows of the table by calling df.sample(5): . objID ra dec metallicity . 15932 1237654601557999788 | 137.603036 | 3.508882 | 8.819281 | . 11276 1237651067353956485 | 189.238856 | 65.712160 | 8.932211 | . 44391 1237661070317322521 | 139.110354 | 10.996989 | 8.410136 | . 121202 1237671990262825066 | 199.912000 | 11.290724 | 8.980943 | . 47280 1237661351633486003 | 184.796897 | 56.327473 | 8.891125 | . This means that a galaxy with objID 1237654601557999788 is located at RA = 137.603036 deg, Dec = 3.508882 deg, and has a metallicity of $Z$ = 8.819281. Our directory structure is such that the corresponding image is stored in {ROOT}/images/1237660634922090677.jpg, where ROOT is the path to project repository. . A tree-view from our {ROOT} directory might look like this: . . ├── data │ └── master.csv ├── images │   ├── 1237654601557999788.jpg │   ├── 1237651067353956485.jpg │   └── [...] └── notebooks └── training-a-cnn.ipynb . We are ready to set up our DataBlock, which is a core fastai construct for handling data. The process is both straightforward and extremely powerful, and comprises a few steps: . Define the inputs and outputs in the blocks argument | Specify how to get your inputs (get_x) and outputs (get_y) | Decide how to split the data into training and validation sets (splitter) | Define any CPU-level transformations (item_tfms) and GPU-level transformation (batch_tfms) used for preprocessing or augmenting your data. | . Before going into the details for each component, here is the code in action: . dblock = DataBlock( blocks=(ImageBlock, RegressionBlock), get_x=ColReader([&#39;objID&#39;], pref=f&#39;{ROOT}/images/&#39;, suff=&#39;.jpg&#39;), get_y=ColReader([&#39;metallicity&#39;]), splitter=RandomSplitter(0.2), item_tfms=[CropPad(144), RandomCrop(112)], batch_tfms=aug_transforms(max_zoom=1., flip_vert=True, max_lighting=0., max_warp=0.) + [Normalize], ) . Okay, now let&#39;s take a look at each part. . Input and output blocks . First, we want to make use of the handy ImageBlock class for handling our input images. Since we&#39;re using galaxy images in the JPG format, we can rely on the PIL backend of ImageBlock to open the images efficiently. If, for example, we instead wanted to use images in the astronomical FITS format, we could extend the TensorImage class and define the following bit of code: . #collapse-hide class FITSImage(TensorImage): @classmethod def create(cls, filename, chans=None, **kwargs) -&gt; None: &quot;&quot;&quot;Create FITS format image by using Astropy to open the file, and then applying appropriate byte swaps and flips to get a Pytorch Tensor. &quot;&quot;&quot; return cls( torch.from_numpy( astropy.io.fits.getdata(fn).byteswap().newbyteorder() ) .flip(0) .float() ) def show(self, ctx=None, ax=None, vmin=None, vmax=None, scale=True, title=None): &quot;&quot;&quot;Plot using matplotlib or your favorite program here!&quot;&quot;&quot; continue FITSImage.create = Transform(FITSImage.create) def FITSImageBlock(): &quot;&quot;&quot;A FITSImageBlock that can be used in the fastai2 DataBlock API. &quot;&quot;&quot; return TransformBlock(partial(FITSImage.create)) . . For our task, the vanilla ImageBlock will suffice. . We also want to define an output block, which will be a RegressionBlock for our task (note that it handles both single- and multi-variable regression). If, for another problem, we wanted to do a categorization problem, then we&#39;d intuitively use the CategoryBlock. Some other examples of the DataBlock API can be found in the documentation. . We can pass in these arguments in the form of a tuple: blocks=(ImageBlock, RegressionBlock). . Input and output object getters . Next, we want to be able to access the table, df, which contain the columns objID and metallicity. As we&#39;ve discussed above, each galaxy&#39;s objID can be used to access the JPG image on disk, which is stored at {ROOT}/images/{objID}.jpg. Fortunately, this is easy to do with the fastai ColumnReader method! We just have to supply it with the column name (objID), a prefix ({ROOT}/images/), and a suffix (.jpg); since the prefix/suffix is only used for file paths, the function knows that the file needs to be opened (rather than interpreting it as a string). So far we have: . get_x=ColReader([&#39;objID&#39;], pref=f&#39;{ROOT}/images/&#39;, suff=&#39;.jpg&#39;) . The targets are stored in metallicity, so we can simply fill in the get_y argument: . get_y=ColReader([&#39;metallicity&#39;]) . (At this point, we haven&#39;t yet specified that df is the DataFrame we&#39;re working with. The DataBlock object knows how to handle the input/output information, but isn&#39;t able to load it until we provide it with df -- that will come later!) . Splitting the data set . For the sake of simplicity, we&#39;ll just randomly split our data set using the aptly named RandomSplitter function. We can provide it with a number between 0 and 1 (corresponding to the fraction of data that will become the validation set), and also a random seed if we wish. In practice, this might look like: . splitter=RandomSplitter(0.2, seed=56) . Transformations and data augmentation . Next, I&#39;ll want to determine some data augmentation transformations. These are handy for varying our image data: crops, flips, and rotations can be applied at random using fastai&#39;s aug_transforms() in order to dramatically expand our data set. Even though we have &gt;100,000 unique galaxy images, our CNN model will contain millions of trainable parameters. Augmenting the data set will be especially valuable for mitigating overfitting. . Translations, rotations, and reflections to our images should not change the properties of our galaxies. However, we won&#39;t want to zoom in and out of the images, since that might impact CNN&#39;s ability to infer unknown (but possibly important) quantities such as the galaxies&#39; intrinsic sizes. Similarly, color shifts or image warps may alter the star formation properties or stellar structures of the galaxies, so we don&#39;t want to mess with that. . We will center crop the image to $144 times 144$ pixels using CropPad(), which reduces some of the surrounding black space (and other galaxies) near the edges of the images. We will then apply a $112 times 112$-pixel RandomCrop() for some more translational freedom. This first set of image crop transformations, item_tfms, will be performed on images one by one using a CPU. Afterwards, the cropped images (which should all be the same size) will be loaded onto the GPU. At this stage, data augmentation transforms will be performed along with image normalization, which rescales the intensities in each channel so that they have zero mean and unit variance. The second set of transformations, batch_tfms, will be applied one batch at a time on the GPU. . item_tfms=[CropPad(144), RandomCrop(112)] batch_tfms=aug_transforms(max_zoom=1., flip_vert=True, max_lighting=0., max_warp=0.) + [Normalize] . . Note: Normalize will pull the batch statistics from your images, and apply it any time you load in new data (see below). Sometimes this can lead to unintended consequences, for example, if you&#8217;re loading in a test data set which is characterized by different image statistics. In that case, I recommend saving your batch statistics and then using them later, e.g., Normalize.from_stats(*image_statistics). . Putting it all together and loading the data . We&#39;ve now gone through each of the steps, but we haven&#39;t yet loaded the data! ImageDataLoaders has a class method called from_dblock() that loads everything in quite nicely if we give it a data source. We can pass along the DataBlock object that we&#39;ve constructed, the DataFrame df, the file path ROOT, and a batch size. We&#39;ve set the batch size bs=128 because that fits on the GPU, and it ensures speedy training, but I&#39;ve found that values between 32 and 128 often work well. . dls = ImageDataLoaders.from_dblock(dblock, df, path=ROOT, bs=128) . Once this is functional, we can view our data set! As we can see, the images have been randomly cropped such that the galaxies are not always in the center of the image. Also, much of the surrounding space has been cropped out. . dls.show_batch(nrows=2, ncols=4) . Pardon the excessive nubmer of significant figures. We can fix this up by creating custom classes extending Transform and ShowTitle, but this is beyond the scope of the current project. Maybe I&#39;ll come back to this in a future post! . Neural network architecture and optimization . . There&#39;s no way that I can describe all of the tweaks and improvements that machine learning researchers have made in the past couple of years, but I&#39;d like to highlight a few that really help out our cause. We need to use some kind of residual CNNs (or resnets), introduced by Kaiming He et al. (2015). Resnets outperform previous CNNs such as the AlexNet or VGG architectures because they can leverage gains from &quot;going deeper&quot; (i.e., by extending the resnets with additional layers). The paper is quite readable and interesting, and there are plenty of other works explaining why resnets are so successful (e.g., a blog post by Anand Saha and a deep dive into residual blocks by He et al.). . . In fastai, we can instantiate a 34-layer enhanced resnet model by using model = xresnet34(). We could have created a 18-layer model with model = xresnet18(), or even defined our own custom 9-layer resnet using . xresnet9 = XResNet(ResBlock, expansion=1, layers=[1, 1, 1, 1]) model = xresnet9() . But first, we need to set the number of outputs. By default, these CNNs are suited for the ImageNet classification challenge, and so there are 1000 outputs. Since we&#39;re performing single-variable regression, the number of outputs (n_out) should be 1. Our DataLoaders class, dls, already knows this and has stored the value 1 in dls.c. . Okay, let&#39;s make our model for real: . model = xresnet34(n_out=dls.c, sa=True, act_cls=MishCuda) . So why did I say that we&#39;re using an &quot;enhanced&quot; resnet -- an &quot;xresnet&quot;? And what does sa=True and act_cls=MishCuda mean? I&#39;ll describe these tweaks below. . A more powerful resnet . The &quot;bag of tricks&quot; paper by Tong He et al. (2018) summarizes many small tweaks that can be combined to dramatically improve the performance of a CNN. They describe several updates to the resnet model architecture in Section 4 of their paper. The fastai library takes these into account, and also implements a few other tweaks, in order to increase performance and speed. I&#39;ve listed some of them below: . The CNN stem (first few layers) is updated using efficient $3 times 3$ convolutions rather than a single expensive layer of $7 times 7$ convolutions. | Residual blocks are changed so that $1 times 1$ convolutions don&#39;t skip over useful information. This is done by altering the order of convolution strides in one path of the downsampling block, and adding a pooling layer in the other path (see Figure 2 of He et al. 2018). | The model concatenates the outputs of both AveragePool and MaxPool layers (using AdaptiveConcatPool2d) rather than using just one. | . Some of these tweaks are described in greater detail in Chapter 14 of the forthcoming fastai book, &quot;Deep Learning for Coders with fastai and Pytorch&quot; (which can be purchased on Amazon). . Self-attention layers . The concept of attention has gotten a lot of, well, attention in deep learning, particularly in natural language processing (NLP). This is because the attention mechanism is a core part of the Transformer architecture, which has revolutionized our ability to learn from text data. I won&#39;t cover the Transformer architecture or NLP in this post, since it&#39;s way out of scope, but suffice it to say that lots of deep learning folks are interested in this idea. . . The attention mechanism allows a neural network layer to encode interactions from inputs on scales larger than the size of a typical convolutional filter. Self-attention is simply when these relationships, encoded via a query/key/value system, are applied using the same input. As a concrete example, self-attention added to CNNs in our scenario -- estimating metallicity from galaxy images -- may allow the network to learn morphological features that often require long-range dependencies, such as the orientation and position angle of a galaxy. . In fastai, we can set sa=True when initializing a CNN in order to get the self-attention layers! . Another way to let a CNN process global information is to use Squeeze-and-Excitation Networks, which are also included in fastai. Or, one could even entirely replace convolutions with self-attention. But we&#39;re starting to get off-topic... . The Mish activation function . Typically, the Rectified Linear Unit (ReLU) is the non-linear activation function of choice for nearly all deep learning tasks. It is both cheap to compute and simple to understand: ReLU(x) = max(0, x). . That was all before Diganta Misra introduced us to the Mish activation function -- as an undergraduate researcher! He also wrote a paper and summarizes some of the reasoning behind it in a forum post. Less Wright, from the fastai community, shows that it performs extremely well in several image classification challenges. I&#39;ve also found that Mish is perfect as a drop-in replacement for ReLU in regression tasks. . The intuition behind the Mish activation function&#39;s success is similar to the reason why resnets perform so well: the loss landscape becomes smoother and thereby easier to explore. ReLU is non-differentiable at the origin, causing steep spikes in the loss. Mish resembles another activation function, Swish, in that neither it nor its derivative is monotonic; this seems to lead to more complex and nuanced behavior during training. However, it&#39;s not clear (from a theoretical perspective) why such activation functions empirically perform so well. . Although Mish is a little bit slower than ReLU, a CUDA implementation helps speed things up a bit. We need to pip install it and then import it with from mish_cuda import MishCuda. Then, we can substitute it into the model when initializing our CNN using act_cls=MishCuda. . RMSE loss . Next we want to select a loss function. The mean squared error (MSE) is suitable for training the network, but we can more easily interpret the root mean squared error (RMSE). We need to create a function to compute the RMSE loss between predictions p and true metalllicity values y. . (Note that we use .view(-1) to flatten our Pytorch Tensor objects since we&#39;re only predicting a single variable.) . def root_mean_squared_error(p, y): return torch.sqrt(F.mse_loss(p.view(-1), y.view(-1))) . Ranger: a combined RAdam + LookAhead optimzation function . Around mid-2019, we saw two new papers regarding the stability of training neural networks: LookAhead and Rectified Adam (RAdam). Both papers feature novel optimizers that address the problem of excess variance during training. LookAhead mitigates the variance problem by scouting a few steps ahead, and then choosing how to optimally update the model&#39;s parameters. RAdam adds a term while computing the adaptive learning rate in order to address training instabilities (see, e.g., the original Adam optimizer). . Less Wright quickly realized that these two optimizers could be combined. His ranger optimizer is the product of these two papers (and now also includes a new tweak, gradient centralization, by default). I have found ranger to give excellent results using empirical tests. . So, now we&#39;ll put everything together in a fastai Learner object: . learn = Learner( dls, model, opt_func=ranger, loss_func=root_mean_squared_error ) . Selecting a learning rate . Fastai offers a nice feature for determining an optimal learning rate, taken from Leslie Smith (2015). All we have to do is call learn.lr_find(). . The idea is to begin feeding your CNN batches of data, while exponentially increasing learning rates (i.e., step sizes) and monitoring the loss. At some point the loss will bottom out, and then begin to increase and diverge wildly, which is a sign that the learn rate is now too high. . . Generally, before the loss starts to diverge, the learning rate will be suitable for the loss to steadily decrease. We can generally read an optimal learning rate off the plot -- the suggested learning rate is around $0.03$ (since that is about an order of magnitude below the learning rate at which the loss &quot;bottoms out&quot; and is also where the loss is decreasing most quickly). I tend to choose a slightly lower learning rate (here I&#39;ll select $0.01$), since that seems to work better for my regression problems. . learn.lr_find() . SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.02290867641568184) . Training the neural network with a &quot;one-cycle&quot; schedule . Finally, now that we&#39;ve selected a learning rate ($0.01$), we can train for a few epochs (remember that one epoch is just a run-through using all of our training data, and we send in one batch of 64 images at a time). Sometimes, researchers simply train at a particular learning rate and wait until the results converge, and then lower the learning rate in order for the model to continue learning. This is because the model needs some serious updates toward the beginning of training (given that it has been initialized with random weights), and then needs to start taking smaller steps once its weights are in the right ballpark. Traditionally, this makes it very expensive to train models because they fail to reach converge for a long time. . Fastai offers a few optimization schedules, which involve altering the learning rate over the course of training. The two most promising are called fit_flat_cos and fit_one_cycle (see more here). I&#39;ve found that fit_flat_cos tends to work better for classification tasks, while fit_one_cycle tends to work better for regression problems. Either way, the empirical results are fantastic -- especially coupled with the Ranger optimizer and all of the other tweaks we&#39;ve discussed. . learn.fit_one_cycle(7, 1e-2) . epoch train_loss valid_loss time . 0 | 0.142891 | 0.368407 | 01:57 | . 1 | 0.165887 | 0.195835 | 01:56 | . 2 | 0.131631 | 0.151854 | 01:57 | . 3 | 0.107358 | 0.099996 | 01:57 | . 4 | 0.096770 | 0.088452 | 01:56 | . 5 | 0.091173 | 0.086019 | 01:57 | . 6 | 0.088538 | 0.085721 | 01:56 | . Here we train for only seven epochs, which took under 14 minutes of training on a single NVIDIA P100 GPU, and achieve a validation loss of 0.086 dex. In our published paper, we were able to reach a RMSE of 0.085 dex in under 30 minutes of training, but that wasn&#39;t from a randomly initialized CNN -- we were using transfer learning then! Here we can accomplish similar results, without pre-training, in only half the time. . We can visualize the training and validation losses. The x-axis shows the number of training iterations (i.e., batches), and the y-axis shows the RMSE loss. . learn.recorder.plot_loss() plt.ylim(0, 0.4); . Evaluating our results . Finally, we&#39;ll perform another round of data augmentation on the validation set in order to see if the results improve. This can be done using learn.tta(), where TTA stands for test-time augmentation. . preds, trues = learn.tta() . Note that we&#39;ll want to flatten these Tensor objects and convert them to numpy arrays, e.g., preds = np.array(preds.view(-1)). At this point, we can plot our results. Everything looks good! . It appears that we didn&#39;t get a lower RMSE using TTA, but that&#39;s okay. TTA is usually worth a shot after you&#39;ve finished training, since evaluating the neural network is relatively quick. . Summary . In summary, we were able to train a deep convolutional neural network to predict galaxy metallicity from three-color images in under 15 minutes. Our data set contained over 100,000 galaxies, so this was no easy feat! Data augmentation, neural network architecture design, and clever optimization tricks were essential for improving performance. With these tools in hand, we can quickly adapt our methodology to tackle many other kinds of problems! . fastai2 is a powerful high-level library that extends Pytorch and is easy to use/customize. At the moment, the documentation is still a bit lacking, but that&#39;s okay -- it&#39;s still under active development! One big takeaway is that fastai, which is all about democratizing AI, makes deep learning more accessible than ever before. . Acknowledgments: I want to thank fastai core development team, Jeremy Howard and Sylvain Gugger, as well as other contributors and invaluable members of the community, including Less Wright, Diganta Misra, and Zachary Mueller. I also want to acknowledge Google for their support via GCP credits for academic research. Finally, I want to give a shout out to Steven Boada, my original collaborator and co-author on our paper. .",
            "url": "https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/computer%20vision/fastai/2020/05/26/training-a-deep-cnn.html",
            "relUrl": "/galaxies/astrophysics/deep%20learning/computer%20vision/fastai/2020/05/26/training-a-deep-cnn.html",
            "date": " • May 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Deep learning with galaxy images",
            "content": "My plunge into deep learning . . Two years ago, I was getting close to the end of my astrophysics PhD program. I had always been interested in statistical methods and machine learning, but never made the leap into deep learning territory. Andrew Ng&#39;s legendary machine learning course was an excellent introduction to building simple neural networks from scratch, but I wasn&#39;t sure what to do next. What I really needed was an astrophysics research project! . Around this time, I was fortunate enough to hear about the Fast.ai &quot;Practical Deep Learning for Coders&quot; (2018) course, which introduced deep learning using a top-down approach. The course also served as a guide for using the Fastai codebase, which is built atop Pytorch. I also noticed that some Fastai users were interested in applying their knowledge to the Kaggle GalaxyZoo classification challenge, which used galaxy image cutouts as their primary source of data. Since I was already interested in galaxy evolution, it seemed like a good idea to think about the types of problems that could be solved using similar data sets. . With the right tools in hand, I teamed up with a postdoc, Steven Boada, and began working on my first deep learning project... . Galaxy evolution at a glance . We see galaxies as gravitationally bound collections of stars, gas, and dust. Galaxies grow by forming new stars out of cold gas, most of which is hydrogen, the lightest and most abundant element in the Universe. Over the course of the stars&#39; lifetimes, heavy elements are fused and eventually strewn across the galaxy through a combination of stellar winds and supernova explosions. Since heavy elements can only be produced inside stars, the ratio of heavy-to-light element abundances is a key measurable for understanding the galaxy&#39;s history of star formation and gas accretion. This abundance ratio of heavy-to-light elements is known as the metallicity. Galaxies which have formed more stars tend to also have higher metallicity. . . This galaxy, NGC 99, is a nearby star-forming spiral galaxy. Its blue color indicates the presence of newly formed stars, which burn brightly but haven&#39;t lived long enough to expel lots of heavy elements into the their surroundings. For this reason, we might expect NGC 99 to be relatively low in metallicity. . . NGC 936 is a redder galaxy, signifying that it has not formed any new stars for a while. Nearly all of its massive, short-lived stars have fused heavy elements and dispersed them throughout the interstellar medium. It does not appear that any pristine, mostly hydrogen (i.e., low-metallicity) gas has recently accreted into the galaxy -- since that would trigger a round of star formation marked by bright blue stars -- so we can infer that this galaxy has fairly high metallicity. . The problem: measuring metallicity takes up a lot of telescope time! . Metallicity does a great job of summarizing a galaxy&#39;s evolutionary history, but it&#39;s not so easy to measure. Typically, astronomers measure the ratio of oxygen to hydrogen atoms in a galaxy, and spectroscopic observations are required for inferring these elemental abundances. Spectroscopy takes much more time than imaging, and is not as easy to do for many objects at once! . The solution: making the most of pretty galaxy pictures . The physical processes that determine a galaxy&#39;s metallicity also leave imprints on the galaxy&#39;s morphology. The structure and color of a galaxy provide us with a rich description of its growth and evolution. Thus, it would make sense that image cutouts might contain enough information for estimating a galaxy&#39;s metallicity. . We train a deep convolutional neural network (CNN) to predict the metallicity directly from imaging. The images are queried from the Sloan Digital Sky Survey (SDSS) SkyServer in JPG format, and consisted of $128 times 128$ pixel images in three colors ($i$, $r$, and $g$ bands corresponding to RGB channels). All in all, we grab about 130,000 galaxy images, and set aside approximately 60% for training, 20% for validation, and 20% for testing. . After only 30 minutes of training on a single GPU, we were able to predict the metallicity of any given galaxy to within 0.085 dex (root mean squared error). This means that our hunch was correct: galaxy images are enough for accurately estimating their metallicities! . . In the figure above, $Z$ represents metallicity, and $Z_{ rm pred}$ and $Z_{ rm true}$ are the CNN-predicted and spectroscopic metallicties, respectively. We are showing a few low-metallicity galaxies (top row), high-metallicity galaxies (middle row), and randomly selected galaxies. We find that our previous intuitions are confirmed! . Morphology, mass, and metallicity . As we have discussed, it is well-known that galaxies&#39; star formation and chemical evolution histories are connected. Previous astronomers have measured a strong correlation between the stellar masses and metallicities of galaxies (forming the so-called mass-metallicity relation, or MZR). Although it is observationally difficult to measure the metallicities of other galaxies, it is easy to measure their stellar masses. So how do we know that the neural network isn&#39;t learning the galaxies&#39; stellar masses, and simply converting these into metallicities via the MZR? . We decided to investigate the relationship between galaxy masses and metallicities measured two ways. The original MZR is constructed from observed metallicities (shown in black below), and we also use CNN-predicted metallicities (shown in red below) to reconstruct the MZR. . . The two versions of the MZR are extremely similar! Both the CNN predictions and the optical spectroscopy give metallicities that correlate with stellar mass to within 0.10 dex (i.e., extremely tight scatter). But the optical spectroscopy served as the ground truth for our CNN, so how could it be that the CNN predictions do not add even a little extra scatter into this original relationship? . It appears that the CNN is using morphological information to characterize galaxies in a way that explains some of the variance in the MZR. In other words, the MZR does not represent the intrinsic scatter due to the physics of galaxy formation and evolution; rather, some of this scatter can be reduced by using other information such as the morphology. (Another possibility is that the galaxy&#39;s star formation rate explains some of the scatter, and that we are instead leveraging this information, but this is unlikely given the fact that we do not study the blue/ultraviolet light from these galaxies.) . Summary . In our paper, which was published in Monthly Notices of the Royal Astronomical Society and can be found on NASA ADS and arXiv, we trained a CNN to predict the metallicity directly from optical-wavelength images. The results were far better than we could have imagined at first: we could accurately estimate the metallicity to within 0.085 dex, and we found that the reconstructed mass-metallicity relationship using CNN predictions had extremely narrow scatter (0.10 dex). . These findings imply that morphological information is essential for understanding how galaxies grow and evolve. Although classification systems and simple parameterizations of their morphological features are useful for encoding this information, they are not nearly as flexible as CNNs. . If you&#39;re interested in seeing some of the more technical details, then please stay tuned for my next post (update: here it is)! I&#39;ll be showcasing some of the analysis using the Fastai v2 codebase. Otherwise, take a look at the original Github repository for the paper, or a demo version of the code (which includes a small subset of the data). .",
            "url": "https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/computer%20vision/2020/05/21/exploring-galaxies-with-deep-learning.html",
            "relUrl": "/galaxies/astrophysics/deep%20learning/computer%20vision/2020/05/21/exploring-galaxies-with-deep-learning.html",
            "date": " • May 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello, I’m John F. Wu! . I’m a postdoctoral researcher at the Space Telescope Science Institute. I’m particularly interested in studying galaxies using deep learning and multiwavelength observations. I have spent time at Johns Hopkins University (in same research group as my current one), Rutgers University (for my PhD), and Carnegie Mellon University (for my undergraduate degree). . My home page describes more of my astrophysics work, while this blog might be a bit more focused on deep learning. Get in touch with me on Twitter (@jwuphysics) or via email (jwuphysics@gmail.com) if you want to chat! .",
          "url": "https://jwuphysics.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jwuphysics.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}