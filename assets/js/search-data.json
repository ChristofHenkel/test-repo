{
  
    
        "post0": {
            "title": "Training a deep CNN to learn from galaxy images in 15 minutes",
            "content": "Introduction . In my previous post, I described the problem that we now want to solve. In short, we want to train a convolutional neural network to perform regression. The inputs are images of individual galaxies, for the most part, although sometimes neighboring or background/foreground galaxies get in the picture too. The outputs are metallicities, $Z$, which usually take on a value between 7.8 and 9.4. . The first step, of course, is to actually get the data. Galaxy images can be fetched using calls to the Sloan Digital Sky Survey (SDSS) SkyServer getJpeg cutout service via their RESTful API. For instance, the URL http://skyserver.sdss.org/dr16/SkyserverWS/ImgCutout/getjpeg?ra=39.8486&amp;dec=1.094&amp;scale=1&amp;width=224&amp;height=224 grabs a three-channel, $224 times 224$-pixel JPG image at the coordinates RA = 39.8486 and Dec = 1.094. Galaxy metallicities can be obtained from the SDSS SkyServer using a SQL query. . The code for the original can be found in my Github repo. However, this code (from 2018) used fastai version 0.7, and I want to show an updated version using the new and improved fastai2 codebase. Also, new &quot;best practices&quot; for deep learning and computer vision exist, so I&#39;d like to highlight those updates as well! . Organizing the data using the fastai2 DataBlock API . Suppose that we now have a directory full of galaxy images, and a csv file with the object identifier, coordinates, and metallcity for each galaxy. The csv table can be read using Pandas, so let&#39;s store that in a DataFrame df. . objID ra dec metallicity . 15932 1237654601557999788 | 137.603036 | 3.508882 | 8.819281 | . 11276 1237651067353956485 | 189.238856 | 65.712160 | 8.932211 | . 44391 1237661070317322521 | 139.110354 | 10.996989 | 8.410136 | . 121202 1237671990262825066 | 199.912000 | 11.290724 | 8.980943 | . 47280 1237661351633486003 | 184.796897 | 56.327473 | 8.891125 | . This means that a galaxy with objID 1237660634922090677 is located at RA = 158.326281 deg, Dec = 43.719674 deg, and has a metallicity of $Z$ = 8.6931. Our directory structure is such that the corresponding image is stored in {ROOT}/images/1237660634922090677.jpg, where ROOT is the project repository&#39;s root directory. . We are ready to set up our DataBlock, which is a core fastai construct. The process is both straightforward and extremely powerful, and comprises a few steps: . Define the inputs and outputs in the blocks argument | Specify how to get your inputs (get_x) and outputs (get_y) | Decide how to split the data into training and validation sets (splitter) | Define any CPU-level transformations (item_tfms) and GPU-level transformation (batch_tfms) used for preprocessing or augmenting your data. | . Before going into the details for each component, here is the code in action: . dblock = DataBlock( blocks=(ImageBlock, RegressionBlock), get_x=ColReader([&#39;objID&#39;], pref=f&#39;{ROOT}/images/&#39;, suff=&#39;.jpg&#39;), get_y=ColReader([&#39;metallicity&#39;]), splitter=RandomSplitter(0.2), item_tfms=[CropPad(144), RandomCrop(112)], batch_tfms=aug_transforms(max_zoom=1., flip_vert=True, max_lighting=0., max_warp=0.) + [Normalize], ) . Okay, now let&#39;s take a look at each piece. . Input and output blocks . First, we want to make use of the handy ImageBlock class for handling our input images. Since we&#39;re using galaxy images in the JPG format, we can rely on the PIL backend of ImageBlock to open the images efficiently. If, for example, we instead wanted to use images in the astronomical FITS format, we could extend the TensorImage class and define the following bit of code: . #collapse-hide class FITSImage(TensorImage): @classmethod def create(cls, filename, chans=None, **kwargs) -&gt; None: &quot;&quot;&quot;Create FITS format image by using Astropy to open the file, and then applying appropriate byte swaps and flips to get a Pytorch Tensor. &quot;&quot;&quot; return cls( torch.from_numpy( astropy.io.fits.getdata(fn).byteswap().newbyteorder() ) .flip(0) .float() ) def show(self, ctx=None, ax=None, vmin=None, vmax=None, scale=True, title=None): &quot;&quot;&quot;Plot using matplotlib or your favorite program here!&quot;&quot;&quot; continue FITSImage.create = Transform(FITSImage.create) def FITSImageBlock(): &quot;&quot;&quot;A FITSImageBlock that can be used in the fastai2 DataBlock API. &quot;&quot;&quot; return TransformBlock(partial(FITSImage.create)) . . For our task, the vanilla ImageBlock will suffice. . We also want to define an output block, which will be a RegressionBlock for our task (note that it handles both single- and multi-variable regression). If, for another problem, we wanted to do a categorization problem, then we&#39;d intuitively use the CategoryBlock. Some other examples of the DataBlock API can be found in the documentation. . We can pass in these arguments in the form of a tuple: blocks=(ImageBlock, RegressionBlock). . Input and output object getters . Next, we want to be able to access the table, df, which contain the columns objID and metallicity. As we&#39;ve discussed above, each galaxy&#39;s objID can be used to access the JPG image on disk, which is stored at {ROOT}/images/{objID}.jpg. Fortunately, this is easy to do with the fastai ColumnReader method! We just have to supply it with the column name (objID), a prefix ({ROOT}/images/), and a suffix (.jpg); since the prefix/suffix is only used for file paths, the function knows that the file needs to be opened (rather than interpreting it as a string). So far we have: . get_x=ColReader([&#39;objID&#39;], pref=f&#39;{ROOT}/images/&#39;, suff=&#39;.jpg&#39;) . The targets are stored in metallicity, so we can simply fill in the get_y argument: . get_y=ColReader([&#39;metallicity&#39;]) . (At this point, we haven&#39;t yet specified that df is the DataFrame we&#39;re working with. The DataBlock object knows how to handle the input/output information, but isn&#39;t able to load it until we provide it with df -- that will come later!) . Splitting the data set . For the sake of simplicity, we&#39;ll just randomly split our data set using the aptly named RandomSplitter function. We can provide it with a number between 0 and 1 (corresponding to the fraction of data that will become the validation set), and also a random seed if we wish. In practice, this might look like: . splitter=RandomSplitter(0.2, seed=56) . Transformations and data augmentation . Next, I&#39;ll want to determine some data augmentation transformations. These are handy for varying our image data: crops, flips, and rotations can be applied at random in order to dramatically expand our data set. Even though we have &gt;100,000 unique galaxy images, our CNN model will contain millions of trainable parameters. Augmenting the data set will be especially valuable for mitigating overfitting. . Translations, rotations, and reflections do not change our galaxies physically. However, we won&#39;t want to zoom in and out of the images, since that might impact CNN&#39;s ability to infer unknown (but possibly covariate) quantities such as the distances or sizes of the galaxies. Similarly, color shifts or image warps may alter the star formation properties or stellar structures of the galaxies, so we don&#39;t want to mess with that. . We will center crop the image to $144 times 144$ pixels using CropPad(), which reduces some of the surrounding black space (and other galaxies) near the edges of the images. We will then apply a $112 times 112$-pixel RandomCrop() for some more translational freedom. The augmentation transforms will be performed along with image normalization, which rescales the images so that they have zero mean and unit variance (this is always a good idea). The first set of image crop transformations, item_tfms, will be individually performed on a CPU. The second set of transformations, batch_tfms, will be applied one batch at a time on the GPU. . item_tfms=[CropPad(144), RandomCrop(112)] batch_tfms=aug_transforms(max_zoom=1., flip_vert=True, max_lighting=0., max_warp=0.) + [Normalize] . . Note: Normalize will pull the batch statistics from your images, and apply it any time you load in new data (see below). Sometimes this can lead to unintended consequences, for example, if you&#8217;re loading in a test data set which is characterized by different image statistics. In that case, I recommend saving your batch statistics and then using them later, e.g., Normalize.from_stats(*image_statistics). . Putting it all together and loading the data . We&#39;ve now gone through each of the steps, but we haven&#39;t yet loaded the data! ImageDataLoaders has a class method called from_dblock that loads everything in quite nicely if we supply it the DataBlock object that we&#39;ve constructed, a data source (df--finally!), a file path, and a batch size. We&#39;ve set the batch size bs=128 because that fits on the GPU, but I&#39;ve found that values between 32 and 128 usually work pretty well. . dls = ImageDataLoaders.from_dblock(dblock, df, path=ROOT, bs=128) . Once this is functional, we can view our data set! As we can see, the images have been randomly cropped such that the galaxies are not always in the center of the image. Also, much of the surrounding space has been cropped out. . dls.show_batch(nrows=2, ncols=4) . Pardon the excessive nubmer of significant figures. We can fix this up by creating custom classes extending Transform and ShowTitle, but this is beyond the scope of the current project. Maybe I&#39;ll come back to this in a future post! . Neural network architecture and optimization . There&#39;s no way that I can describe all of the tweaks and improvements that machine learning researchers have made in the past couple of years, but I&#39;d like to highlight a few that really help out our cause. We need to use some kind of residual CNNs (or resnets), introduced by Kaiming He et al. (2015). Resnets outperform previous CNNs such as the AlexNet or VGG architectures because they can leverage gains from &quot;going deeper&quot; (i.e., by extending the resnets with additional layers). The paper is quite readable and interesting, and there are plenty of other works explaining why resnets are so successful (e.g., a blog post by Anand Saha and a deep dive into residual blocks by He et al.). . In fastai2, we can instantiate a 34-layer enhanced resnet model by using model = xresnet34(). We could have selected an 18-layer model with model = xresnet18(), or even defined our own custom 9-layer resnet using . xresnet9 = XResNet(ResBlock, expansion=1, layers=[1, 1, 1, 1]) model = xresnet() . But first, we need to set the number of outputs. Since we&#39;re performing single-variable regression, the number of outputs (specified by the n_out parameter) is 1. Our DataLoaders class, dls, already knows this and has stored it in the property dls.c. . Okay, let&#39;s make our model for real: . model = xresnet34(n_out=dls.c, sa=True, act_cls=MishCuda) . So why did I say that we&#39;re using an &quot;enhanced&quot; resnet -- an &quot;xresnet&quot;? And what does sa=True and act_cls=MishCuda mean? I&#39;ll describe these tweaks below. . A more powerful resnet . Self-attention layers . The Mish activation function . RMSE loss . def root_mean_squared_error(p, y): return torch.sqrt(F.mse_loss(p.reshape(-1), y.reshape(-1))) . Ranger: a combined RAdam + LookAhead optimzation function . learn = Learner( dls, model, opt_func=ranger, loss_func=root_mean_squared_error ) . Selecting a learning rate . learn.lr_find() . SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.02290867641568184) . Training the neural network . learn.fit_one_cycle(7, 1e-2) . epoch train_loss valid_loss time . 0 | 0.142891 | 0.368407 | 01:57 | . 1 | 0.165887 | 0.195835 | 01:56 | . 2 | 0.131631 | 0.151854 | 01:57 | . 3 | 0.107358 | 0.099996 | 01:57 | . 4 | 0.096770 | 0.088452 | 01:56 | . 5 | 0.091173 | 0.086019 | 01:57 | . 6 | 0.088538 | 0.085721 | 01:56 | . We can see that overall our network has done quite well with under 15 minutes of training! . learn.recorder.plot_loss() plt.ylim(0, 0.4); . Evaluating our results . Test time augmentation . preds, trues = learn.tta() . &lt;progress value=&#39;4&#39; class=&#39;&#39; max=&#39;4&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [4/4 00:55&lt;00:00] &lt;progress value=&#39;210&#39; class=&#39;&#39; max=&#39;210&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [210/210 00:13&lt;00:00 0.0885] Text(0, 0.5, &#39;$Z_{ rm pred}$&#39;) .",
            "url": "https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/2020/05/25/_blog_-recreating-Wu-&-Boada-(2019).html",
            "relUrl": "/galaxies/astrophysics/deep%20learning/2020/05/25/_blog_-recreating-Wu-&-Boada-(2019).html",
            "date": " • May 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep learning with galaxy images",
            "content": "My plunge into deep learning . . Two years ago, I was getting close to the end of my astrophysics PhD program. I had always been interested in statistical methods and machine learning, but never made the leap into deep learning territory. Andrew Ng&#39;s legendary machine learning course was an excellent introduction to building simple neural networks from scratch, but I wasn&#39;t sure what to do next. What I really needed was an astrophysics research project! . Around this time, I was fortunate enough to hear about the Fast.ai &quot;Practical Deep Learning for Coders&quot; (2018) course, which introduced deep learning using a top-down approach. The course also served as a guide for using the Fastai codebase, which is built atop Pytorch. I also noticed that some Fastai users were interested in applying their knowledge to the Kaggle GalaxyZoo classification challenge, which used galaxy image cutouts as their primary source of data. Since I was already interested in galaxy evolution, it seemed like a good idea to think about the types of problems that could be solved using similar data sets. . With the right tools in hand, I teamed up with a postdoc, Steven Boada, and began working on my first deep learning project... . A quick introduction to galaxy evolution . We see galaxies as gravitationally bound collections of stars, gas, and dust. Galaxies grow by forming new stars out of cold gas, most of which is hydrogen, the lightest and most abundant element in the Universe. Over the course of the stars&#39; lifetimes, heavy elements are fused and eventually strewn across the galaxy through a combination of stellar winds and supernova explosions. Since heavy elements can only be produced inside stars, the ratio of heavy-to-light element abundances is a key measurable for understanding the galaxy&#39;s history of star formation and gas accretion. This abundance ratio of heavy-to-light elements is known as the metallicity. Galaxies which have formed more stars tend to also have higher metallicity. . . This galaxy, NGC 99, is a nearby star-forming spiral galaxy. Its blue color indicates the presence of newly formed stars, which burn brightly but haven&#39;t lived long enough to expel lots of heavy elements into the their surroundings. For this reason, we might expect NGC 99 to be relatively low in metallicity. . . NGC 936 is a redder galaxy, signifying that it has not formed any new stars for a while. Nearly all of its massive, short-lived stars have fused heavy elements and dispersed them throughout the interstellar medium. It does not appear that any pristine, mostly hydrogen (i.e., low-metallicity) gas has recently accreted into the galaxy -- since that would trigger a round of star formation marked by bright blue stars -- so we can infer that this galaxy has fairly high metallicity. . The problem: measuring metallicity takes up a lot of telescope time! . Metallicity does a great job of summarizing a galaxy&#39;s evolutionary history, but it&#39;s not so easy to measure. Typically, astronomers measure the ratio of oxygen to hydrogen atoms in a galaxy, and spectroscopic observations are required for inferring these elemental abundances. Spectroscopy takes much more time than imaging, and is not as easy to do for many objects at once! . The solution: making the most of pretty galaxy pictures . The physical processes that determine a galaxy&#39;s metallicity also leave imprints on the galaxy&#39;s morphology. The structure and color of a galaxy provide us with a rich description of its growth and evolution. Thus, it would make sense that image cutouts might contain enough information for estimating a galaxy&#39;s metallicity. . We train a deep convolutional neural network (CNN) to predict the metallicity directly from imaging. The images are queried from the Sloan Digital Sky Survey (SDSS) SkyServer in JPG format, and consisted of $128 times 128$ pixel images in three colors ($i$, $r$, and $g$ bands corresponding to RGB channels). All in all, we grab about 130,000 galaxy images, and set aside approximately 60% for training, 20% for validation, and 20% for testing. . After only 30 minutes of training on a single GPU, we were able to predict the metallicity of any given galaxy to within 0.085 dex (root mean squared error). This means that our hunch was correct: galaxy images are enough for accurately estimating their metallicities! . . In the figure above, $Z$ represents metallicity, and $Z_{ rm pred}$ and $Z_{ rm true}$ are the CNN-predicted and spectroscopic metallicties, respectively. We are showing a few low-metallicity galaxies (top row), high-metallicity galaxies (middle row), and randomly selected galaxies. We find that our previous intuitions are confirmed! . Morphology, mass, and metallicity . As we have discussed, it is well-known that galaxies&#39; star formation and chemical evolution histories are connected. Previous astronomers have measured a strong correlation between the stellar masses and metallicities of galaxies (forming the so-called mass-metallicity relation, or MZR). Although it is observationally difficult to measure the metallicities of other galaxies, it is easy to measure their stellar masses. So how do we know that the neural network isn&#39;t learning the galaxies&#39; stellar masses, and simply converting these into metallicities via the MZR? . We decided to investigate the relationship between galaxy masses and metallicities measured two ways. The original MZR is constructed from observed metallicities (shown in black below), and we also use CNN-predicted metallicities (shown in red below) to reconstruct the MZR. . . The two versions of the MZR are extremely similar! Both the CNN predictions and the optical spectroscopy give metallicities that correlate with stellar mass to within 0.10 dex (i.e., extremely tight scatter). But the optical spectroscopy served as the ground truth for our CNN, so how could it be that the CNN predictions do not add even a little extra scatter into this original relationship? . It appears that the CNN is using morphological information to characterize galaxies in a way that explains some of the variance in the MZR. In other words, the MZR does not represent the intrinsic scatter due to the physics of galaxy formation and evolution; rather, some of this scatter can be reduced by using other information such as the morphology. (Another possibility is that the galaxy&#39;s star formation rate explains some of the scatter, and that we are instead leveraging this information, but this is unlikely given the fact that we do not study the blue/ultraviolet light from these galaxies.) . Summary . In our paper, which was published in Monthly Notices of the Royal Astronomical Society and can be found on NASA ADS and arXiv, we trained a CNN to predict the metallicity directly from optical-wavelength images. The results were far better than we could have imagined at first: we could accurately estimate the metallicity to within 0.085 dex, and we found that the reconstructed mass-metallicity relationship using CNN predictions had extremely narrow scatter (0.10 dex). . These findings imply that morphological information is essential for understanding how galaxies grow and evolve. Although classification systems and simple parameterizations of their morphological features are useful for encoding this information, they are not nearly as flexible as CNNs. . If you&#39;re interested in seeing some of the more technical details, then please stay tuned for my next post! I&#39;ll be showcasing some of the analysis, updated using the Fastai v2 codebase. Otherwise, take a look at the original Github repository for the paper, or a demo version of the code (which includes a small subset of the data). .",
            "url": "https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/2020/05/21/exploring-galaxies-with-deep-learning.html",
            "relUrl": "/galaxies/astrophysics/deep%20learning/2020/05/21/exploring-galaxies-with-deep-learning.html",
            "date": " • May 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello, I’m John F. Wu! . I’m a postdoctoral researcher at Johns Hopkins University and Space Telescope Science Institute. I’m particularly interested in studying galaxies using deep learning and multiwavelength observations. . My home page describes more of my astrophysics work, while this blog might be a bit more focused on deep learning. .",
          "url": "https://jwuphysics.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jwuphysics.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}